{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T21:21:50.315791Z",
     "start_time": "2025-01-22T21:21:49.805909Z"
    }
   },
   "source": [
    "# Importar bibliotecas necesarias\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T21:22:06.136214Z",
     "start_time": "2025-01-22T21:21:50.316844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_signal(signal):\n",
    "\n",
    "    # Parámetros teóricos\n",
    "    max_theoretical_value = 230 * np.sqrt(2)\n",
    "\n",
    "    # Media de la señal\n",
    "    mean_value = np.mean(signal)\n",
    "\n",
    "    # Datos sin sesgo\n",
    "    unbias_data = signal - mean_value\n",
    "    unbias_data_2 = unbias_data ** 2\n",
    "    unbias_data_3 = unbias_data_2 * unbias_data\n",
    "    unbias_data_4 = unbias_data_3 * unbias_data\n",
    "\n",
    "    # Cálculo de características\n",
    "    variance = np.var(unbias_data)  # Varianza\n",
    "    skewness = np.mean(unbias_data_3) / (variance ** 1.5)  # Asimetría\n",
    "    kurtosis = np.mean(unbias_data_4) / (variance ** 2) - 3  # Curtosis\n",
    "    thd = np.sqrt(np.sum(np.abs(np.fft.fft(signal)[2:4])) / np.abs(np.fft.fft(signal)[1]))  # Distorsión armónica total\n",
    "    rms = np.sqrt(np.mean(signal ** 2))  # Valor RMS\n",
    "    crest_factor = np.max(signal) / rms  # Factor de cresta\n",
    "\n",
    "    # Devuelve todas las características en un vector\n",
    "    return np.array([variance, skewness, kurtosis, thd, crest_factor])\n",
    "\n",
    "def load_signal(data_path):\n",
    "\n",
    "    # Asignar etiquetas explícitamente\n",
    "    label_mapping = {\n",
    "        \"flicker_signals\": 0,\n",
    "        \"harmonic_signals\": 1,\n",
    "        \"interruption_signals\": 2,\n",
    "        \"original_signals\": 3,\n",
    "        \"sag_signals\": 4,\n",
    "        \"swell_signals\": 5,\n",
    "        \"transient_signals\": 6,\n",
    "    }\n",
    "\n",
    "    # Inicialización de listas para características y etiquetas\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterar por cada tipo de perturbación\n",
    "    for signal_type, label in label_mapping.items():\n",
    "        signal_type_path = os.path.join(data_path, signal_type)\n",
    "\n",
    "        if os.path.isdir(signal_type_path):\n",
    "            for subset in [\"train\", \"test\", \"val\"]:\n",
    "                subset_path = os.path.join(signal_type_path, subset)\n",
    "\n",
    "                if os.path.exists(subset_path):\n",
    "                    for filename in os.listdir(subset_path):\n",
    "                        if filename.endswith(\".npy\"):\n",
    "                            file_path = os.path.join(subset_path, filename)\n",
    "\n",
    "                            # Cargar la señal\n",
    "                            signal = np.load(file_path)\n",
    "\n",
    "                            # Procesar la señal y extraer características\n",
    "                            feature_vector = preprocess_signal(signal)\n",
    "\n",
    "                            # Agregar las características y etiquetas a las listas\n",
    "                            features.append(feature_vector)\n",
    "                            labels.append(label)\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Ejemplo de uso\n",
    "data_path = \"data\"  # Ajusta esta ruta según sea necesario\n",
    "features, labels = load_signal(data_path)\n",
    "\n",
    "print(f\"Características extraídas: {features.shape}\")\n",
    "print(f\"Etiquetas extraídas: {labels.shape}\")\n",
    "\n",
    "\n",
    "# Contar las etiquetas únicas en los datos originales \n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "print(\"Etiquetas únicas:\", unique_labels)\n",
    "print(\"Distribución de señales por categoría:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Clase {label}: {count} señales\")\n",
    "\n",
    "# Verificar que la clase 3 está presente\n",
    "if 3 not in unique_labels: \n",
    "    print(\"Error: La clase 3 no está presente en los datos originales.\")"
   ],
   "id": "f41ae91da761ba91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características extraídas: (75600, 5)\n",
      "Etiquetas extraídas: (75600,)\n",
      "Etiquetas únicas: [0 1 2 3 4 5 6]\n",
      "Distribución de señales por categoría:\n",
      "Clase 0: 10800 señales\n",
      "Clase 1: 10800 señales\n",
      "Clase 2: 10800 señales\n",
      "Clase 3: 10800 señales\n",
      "Clase 4: 10800 señales\n",
      "Clase 5: 10800 señales\n",
      "Clase 6: 10800 señales\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T21:43:23.633825Z",
     "start_time": "2025-01-22T21:22:06.136811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definir los rangos de hiperparámetros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],       # Valores bajos a altos de C\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Gamma ajustado\n",
    "    'kernel': ['rbf']             # Mantener kernel RBF\n",
    "}\n",
    "\n",
    "# Crear el modelo SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Configurar búsqueda en cuadrícula\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Entrenar el modelo y realizar búsqueda de hiperparámetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros y rendimiento\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (validación cruzada):\", grid_search.best_score_)\n",
    "\n",
    "# Usar el mejor modelo para predicciones\n",
    "best_svm = grid_search.best_estimator_\n",
    "predicciones = best_svm.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, predicciones)\n",
    "print(f\"Accuracy en el conjunto de prueba: {accuracy:.2f}\")\n",
    "\n",
    "# Generar el informe de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, predicciones))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "conf_matrix = confusion_matrix(y_test, predicciones)\n",
    "print(conf_matrix)"
   ],
   "id": "edfd909d7eebd442",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "Mejor puntuación (validación cruzada): 0.9825727513227515\n",
      "Accuracy en el conjunto de prueba: 0.98\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2154\n",
      "           1       0.96      0.96      0.96      2134\n",
      "           2       0.97      1.00      0.98      2193\n",
      "           3       1.00      1.00      1.00      2158\n",
      "           4       0.99      0.96      0.97      2146\n",
      "           5       0.98      0.98      0.98      2187\n",
      "           6       1.00      0.99      0.99      2148\n",
      "\n",
      "    accuracy                           0.98     15120\n",
      "   macro avg       0.98      0.98      0.98     15120\n",
      "weighted avg       0.98      0.98      0.98     15120\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[2154    0    0    0    0    0    0]\n",
      " [  23 2041    2    1   21   46    0]\n",
      " [   0    3 2188    1    1    0    0]\n",
      " [   0    2    0 2156    0    0    0]\n",
      " [   0   18   69    0 2059    0    0]\n",
      " [   1   40    0    1    0 2141    4]\n",
      " [   0   19    0    1    0    2 2126]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T21:43:23.635923Z",
     "start_time": "2025-01-22T21:43:23.634527Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c2a76db380819e7d",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
