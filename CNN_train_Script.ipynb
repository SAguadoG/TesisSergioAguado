{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN Train Script",
   "id": "b9fa683acce4e34d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Este es el archivo donde se hará una prueba de clasificador automático mediante CNN (Convolutional Neural Network).",
   "id": "79203c842a7fc34a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "De manera independiente se van a cargar las librerías necesarias para el script.\n",
    "A la hora de realizar este script, Tensorflow/Keras no es aún compatible con Python 3.12, así que se usará la versión de Python 3.11.6"
   ],
   "id": "9e81c3a5c39d3c85"
  },
  {
   "cell_type": "code",
   "id": "a7b233b096c81103",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T19:26:19.947414Z",
     "start_time": "2025-01-21T19:26:19.944469Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para puntualizar ahora mismo, se añadirán mas tipos en un futuro.\n",
    "1. original_signals → Etiqueta 0.\n",
    "2. flicker_signals → Etiqueta 1.\n",
    "3. harmonic_signals → Etiqueta 2.\n",
    "4. Interruption_signals → Etiqueta 3.\n",
    "5. original_signals → Etiqueta 4.\n",
    "6. Swell_signals → Etiqueta 5.\n",
    "7. transient_signals → Etiqueta 6."
   ],
   "id": "6d9872572e7fe718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:26:36.456280Z",
     "start_time": "2025-01-21T19:26:19.955355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_signal(signal):\n",
    "\n",
    "    # Parámetros teóricos\n",
    "    max_theoretical_value = 230 * np.sqrt(2)\n",
    "\n",
    "    # Media de la señal\n",
    "    mean_value = np.mean(signal)\n",
    "\n",
    "    # Datos sin sesgo\n",
    "    unbias_data = signal - mean_value\n",
    "    unbias_data_2 = unbias_data ** 2\n",
    "    unbias_data_3 = unbias_data_2 * unbias_data\n",
    "    unbias_data_4 = unbias_data_3 * unbias_data\n",
    "\n",
    "    # Cálculo de características\n",
    "    variance = np.var(unbias_data)  # Varianza\n",
    "    skewness = np.mean(unbias_data_3) / (variance ** 1.5)  # Asimetría\n",
    "    kurtosis = np.mean(unbias_data_4) / (variance ** 2) - 3  # Curtosis\n",
    "    thd = np.sqrt(np.sum(np.abs(np.fft.fft(signal)[2:4])) / np.abs(np.fft.fft(signal)[1]))  # Distorsión armónica total\n",
    "    rms = np.sqrt(np.mean(signal ** 2))  # Valor RMS\n",
    "    crest_factor = np.max(signal) / rms  # Factor de cresta\n",
    "\n",
    "    # Devuelve todas las características en un vector\n",
    "    return np.array([variance, skewness, kurtosis, thd, crest_factor])\n",
    "\n",
    "def load_signal(data_path):\n",
    "\n",
    "    # Asignar etiquetas explícitamente\n",
    "    label_mapping = {\n",
    "        \"flicker_signals\": 0,\n",
    "        \"harmonic_signals\": 1,\n",
    "        \"interruption_signals\": 2,\n",
    "        \"original_signals\": 3,\n",
    "        \"sag_signals\": 4,\n",
    "        \"swell_signals\": 5,\n",
    "        \"transient_signals\": 6,\n",
    "    }\n",
    "\n",
    "    # Inicialización de listas para características y etiquetas\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterar por cada tipo de perturbación\n",
    "    for signal_type, label in label_mapping.items():\n",
    "        signal_type_path = os.path.join(data_path, signal_type)\n",
    "\n",
    "        if os.path.isdir(signal_type_path):\n",
    "            for subset in [\"train\", \"test\", \"val\"]:\n",
    "                subset_path = os.path.join(signal_type_path, subset)\n",
    "\n",
    "                if os.path.exists(subset_path):\n",
    "                    for filename in os.listdir(subset_path):\n",
    "                        if filename.endswith(\".npy\"):\n",
    "                            file_path = os.path.join(subset_path, filename)\n",
    "\n",
    "                            # Cargar la señal\n",
    "                            signal = np.load(file_path)\n",
    "\n",
    "                            # Procesar la señal y extraer características\n",
    "                            feature_vector = preprocess_signal(signal)\n",
    "\n",
    "                            # Agregar las características y etiquetas a las listas\n",
    "                            features.append(feature_vector)\n",
    "                            labels.append(label)\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Ejemplo de uso\n",
    "data_path = \"data\"  # Ajusta esta ruta según sea necesario\n",
    "features, labels = load_signal(data_path)\n",
    "\n",
    "print(f\"Características extraídas: {features.shape}\")\n",
    "print(f\"Etiquetas extraídas: {labels.shape}\")\n",
    "\n",
    "\n",
    "# Contar las etiquetas únicas en los datos originales \n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "print(\"Etiquetas únicas:\", unique_labels)\n",
    "print(\"Distribución de señales por categoría:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Clase {label}: {count} señales\")\n",
    "\n",
    "# Verificar que la clase 3 está presente\n",
    "if 3 not in unique_labels: \n",
    "    print(\"Error: La clase 3 no está presente en los datos originales.\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características extraídas: (75600, 5)\n",
      "Etiquetas extraídas: (75600,)\n",
      "Etiquetas únicas: [0 1 2 3 4 5 6]\n",
      "Distribución de señales por categoría:\n",
      "Clase 0: 10800 señales\n",
      "Clase 1: 10800 señales\n",
      "Clase 2: 10800 señales\n",
      "Clase 3: 10800 señales\n",
      "Clase 4: 10800 señales\n",
      "Clase 5: 10800 señales\n",
      "Clase 6: 10800 señales\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-21T19:26:36.457181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dividir los datos en entrenamiento, validación y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verificar la forma de las características y etiquetas\n",
    "print(f\"Características de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Características de validación: {X_val.shape}\")\n",
    "print(f\"Características de prueba: {X_test.shape}\")\n",
    "print(f\"Etiquetas de entrenamiento: {y_train.shape}\")\n",
    "print(f\"Etiquetas de validación: {y_val.shape}\")\n",
    "print(f\"Etiquetas de prueba: {y_test.shape}\")\n",
    "\n",
    "# Si las etiquetas no están en formato one-hot, convertirlas\n",
    "# Esto es necesario solo si decides usar categorical_crossentropy\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=7)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes=7)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=7)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential([\n",
    "    Dense(64, activation='tanh',input_shape=(5,),kernel_regularizer=l2(0.001)),  # Capa de entrada\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='tanh'),  # Capa oculta 1\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='tanh'),  # Capa oculta 2 \n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh'),  # Capa oculta 3\n",
    "    Dense(7, activation='softmax')  # Capa de salida (7 clases)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',  # Pérdida para clasificación multiclase (one-hot encoded)\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train_one_hot,\n",
    "                    validation_data=(X_val, y_val_one_hot),\n",
    "                    epochs=50,\n",
    "                    batch_size=32)\n",
    "\n",
    "# Evaluar el modelo en los datos de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Obtener las predicciones del modelo en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
    "\n",
    "# Calcular el informe de clasificación\n",
    "class_report = classification_report(y_test_classes, y_pred_classes)\n",
    "print(\"Informe de Clasificación:\\n\", class_report)\n"
   ],
   "id": "d88dae4836c64e07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características de entrenamiento: (52920, 5)\n",
      "Características de validación: (11340, 5)\n",
      "Características de prueba: (11340, 5)\n",
      "Etiquetas de entrenamiento: (52920,)\n",
      "Etiquetas de validación: (11340,)\n",
      "Etiquetas de prueba: (11340,)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saguadog/Documents/TesisSergioAguado/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-01-21 20:26:36.466758: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2025-01-21 20:26:36.466787: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-01-21 20:26:36.466791: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2025-01-21 20:26:36.466806: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-21 20:26:36.466815: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-01-21 20:26:36.851835: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 15ms/step - accuracy: 0.2300 - loss: 1.9162 - val_accuracy: 0.4380 - val_loss: 1.3171\n",
      "Epoch 2/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 15ms/step - accuracy: 0.4311 - loss: 1.4070 - val_accuracy: 0.6491 - val_loss: 1.0995\n",
      "Epoch 3/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.5166 - loss: 1.2601 - val_accuracy: 0.6809 - val_loss: 1.0012\n",
      "Epoch 4/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.5396 - loss: 1.2409 - val_accuracy: 0.7045 - val_loss: 0.9596\n",
      "Epoch 5/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.5308 - loss: 1.3736 - val_accuracy: 0.6742 - val_loss: 1.0032\n",
      "Epoch 6/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 16ms/step - accuracy: 0.4949 - loss: 1.7940 - val_accuracy: 0.6334 - val_loss: 1.1359\n",
      "Epoch 7/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.4513 - loss: 2.5096 - val_accuracy: 0.6564 - val_loss: 1.6293\n",
      "Epoch 8/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.4288 - loss: 3.3020 - val_accuracy: 0.6596 - val_loss: 1.3851\n",
      "Epoch 9/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.3954 - loss: 3.9176 - val_accuracy: 0.4442 - val_loss: 1.8864\n",
      "Epoch 10/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 15ms/step - accuracy: 0.3732 - loss: 4.2206 - val_accuracy: 0.6052 - val_loss: 1.8949\n",
      "Epoch 11/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 16ms/step - accuracy: 0.3396 - loss: 4.4092 - val_accuracy: 0.4510 - val_loss: 2.3619\n",
      "Epoch 12/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 16ms/step - accuracy: 0.3301 - loss: 4.5225 - val_accuracy: 0.4048 - val_loss: 3.4839\n",
      "Epoch 13/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 16ms/step - accuracy: 0.3122 - loss: 4.4393 - val_accuracy: 0.4112 - val_loss: 2.2244\n",
      "Epoch 14/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 15ms/step - accuracy: 0.3067 - loss: 4.2815 - val_accuracy: 0.3614 - val_loss: 3.2028\n",
      "Epoch 15/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 15ms/step - accuracy: 0.3044 - loss: 4.2996 - val_accuracy: 0.3570 - val_loss: 4.1244\n",
      "Epoch 16/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 15ms/step - accuracy: 0.2989 - loss: 4.3979 - val_accuracy: 0.3325 - val_loss: 4.1137\n",
      "Epoch 17/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 16ms/step - accuracy: 0.3030 - loss: 4.9575 - val_accuracy: 0.3717 - val_loss: 3.1952\n",
      "Epoch 18/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 17ms/step - accuracy: 0.2965 - loss: 5.8643 - val_accuracy: 0.3630 - val_loss: 2.1550\n",
      "Epoch 19/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 18ms/step - accuracy: 0.2913 - loss: 6.4965 - val_accuracy: 0.3634 - val_loss: 5.1169\n",
      "Epoch 20/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step - accuracy: 0.2964 - loss: 7.2165 - val_accuracy: 0.1942 - val_loss: 11.0875\n",
      "Epoch 21/50\n",
      "\u001B[1m1654/1654\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 20ms/step - accuracy: 0.2933 - loss: 8.7010 - val_accuracy: 0.2489 - val_loss: 8.1444\n",
      "Epoch 22/50\n",
      "\u001B[1m 280/1654\u001B[0m \u001B[32m━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m20s\u001B[0m 15ms/step - accuracy: 0.2783 - loss: 9.4862"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Gráfico de pérdida\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de precisión\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b866329d3d8b905c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
